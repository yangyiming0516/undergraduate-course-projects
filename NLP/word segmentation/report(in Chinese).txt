实验报告：

非结构化感知器：
    选取的特征有x-1,y / x,y / x+1,y / (x-1,x),y / (x,x+1),y,即临近和当前字与标注的关系，和包含当前字的bigram与标注的关系
    其中y为标注序列1/0，1对应该字前应切，0对应不切
    特征向量的维数是（unigram_num*3+bigram_num*2）*2
    使用普通的感知器训练方法，每次给权重向量W应调整的维度进行+-1的调整
    最后对测试集根据W进行标注，并输出分词后的文本
	
    结果：
	训练1次train.txt 
		precision:0.853 recall:0.893 F-Score:0.872
	训练5次train.txt
		precision:0.897 recall:0.923 F-Score:0.910
	训练10次
		precision:0.904 recall:0.923 F-Score:0.913
		

结构化感知器：
    为了能够取得一句话的全局最优解，首先要增加特征的维度，以使前后字的标注产生影响，从而可以利用维特比算法。
    在非结构化感知器的5种特征的基础上，区分当前字和前一个字标注的具体情况，比如（x-1,x）,y -》 （x-1，x），(y-1,y)
    特征的维数上升到（unigram_num*3+bigram_num*2）*4
    根据结构化感知器的训练流程，使用维特比动态规划，求出一句话的全局最优解，并回溯得到具体序列
    将该序列与训练集实际正确序列进行比对，在针对每一位具体对W进行调整，调整幅度依然是+-1
    同样对于测试集通过动态规划的算法求出每句话的标注

     结果：
	训练1次train.txt 
		precision:0.894 recall:0.887 F-Score:0.890
	训练5次train.txt
		precision:0.930 recall:0.930 F-Score:0.927
	训练10次
		precision:0.934 recall:0.925 F-Score:0.930

对比：
    原理上说，结构化感知器通过隐马尔科夫模型，利用了前后字标注之间的关系，增大了特征的维数，效果应该是更好的，实际也是这样，可以说提升非常明显。另一方面，结构化感知器运行的时间也要长得多。

进一步的提升
    事实上，增加特征的数量可以显著提升效果，尤其当引入trigram的时候，但是因为成本问题没有进行尝试。